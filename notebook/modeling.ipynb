{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BiY8w_cFoc7u"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
        "\n",
        "# Validation of the models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "# Preprocessing and trasformations\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZSGqwq8poc7v"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "test_features = pd.read_csv(\"../data/processed/test_features.csv\")\n",
        "test_labels=pd.read_csv(\"../data/processed/test_labels.csv\")\n",
        "train_features=pd.read_csv(\"../data/processed/train_features.csv\")\n",
        "train_labels=pd.read_csv(\"../data/processed/train_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr7iJTJqoc7w",
        "outputId": "865298f5-a3b8-42e6-83f4-bace9661799e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24129 entries, 0 to 24128\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype   \n",
            "---  ------  --------------  -----   \n",
            " 0   income  24129 non-null  category\n",
            "dtypes: category(1)\n",
            "memory usage: 23.8 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6033 entries, 0 to 6032\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype   \n",
            "---  ------  --------------  -----   \n",
            " 0   income  6033 non-null   category\n",
            "dtypes: category(1)\n",
            "memory usage: 6.1 KB\n"
          ]
        }
      ],
      "source": [
        "# Convert object variables into categorical ones for labels dataset\n",
        "\n",
        "train_labels[\"income\"] = pd.Categorical(train_labels[\"income\"])\n",
        "train_labels.info()\n",
        "\n",
        "test_labels[\"income\"] = pd.Categorical(test_labels[\"income\"])\n",
        "test_labels.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfYJythnoc7w",
        "outputId": "32cae31c-833a-4aad-d0dc-bcef8162aba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24129 entries, 0 to 24128\n",
            "Data columns (total 14 columns):\n",
            " #   Column                  Non-Null Count  Dtype   \n",
            "---  ------                  --------------  -----   \n",
            " 0   age                     24129 non-null  int64   \n",
            " 1   workclass               24129 non-null  category\n",
            " 2   fnlwgt                  24129 non-null  int64   \n",
            " 3   education.num           24129 non-null  int64   \n",
            " 4   marital.status          24129 non-null  category\n",
            " 5   occupation              24129 non-null  category\n",
            " 6   relationship            24129 non-null  category\n",
            " 7   race                    24129 non-null  category\n",
            " 8   sex                     24129 non-null  category\n",
            " 9   capital.gain            24129 non-null  int64   \n",
            " 10  capital.loss            24129 non-null  int64   \n",
            " 11  hours.per.week          24129 non-null  int64   \n",
            " 12  native.country.grouped  24129 non-null  category\n",
            " 13  education.grouped       24129 non-null  category\n",
            "dtypes: category(8), int64(6)\n",
            "memory usage: 1.3 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6033 entries, 0 to 6032\n",
            "Data columns (total 14 columns):\n",
            " #   Column                  Non-Null Count  Dtype   \n",
            "---  ------                  --------------  -----   \n",
            " 0   age                     6033 non-null   int64   \n",
            " 1   workclass               6033 non-null   category\n",
            " 2   fnlwgt                  6033 non-null   int64   \n",
            " 3   education.num           6033 non-null   int64   \n",
            " 4   marital.status          6033 non-null   category\n",
            " 5   occupation              6033 non-null   category\n",
            " 6   relationship            6033 non-null   category\n",
            " 7   race                    6033 non-null   category\n",
            " 8   sex                     6033 non-null   category\n",
            " 9   capital.gain            6033 non-null   int64   \n",
            " 10  capital.loss            6033 non-null   int64   \n",
            " 11  hours.per.week          6033 non-null   int64   \n",
            " 12  native.country.grouped  6033 non-null   category\n",
            " 13  education.grouped       6033 non-null   category\n",
            "dtypes: category(8), int64(6)\n",
            "memory usage: 332.5 KB\n"
          ]
        }
      ],
      "source": [
        "# Convert object variables into categorical ones for features dataset\n",
        "\n",
        "categorical_columns = train_features.select_dtypes(include='object').columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    train_features[col] = pd.Categorical(train_features[col])\n",
        "\n",
        "train_features.info()\n",
        "\n",
        "categorical_columns = test_features.select_dtypes(include='object').columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    test_features[col] = pd.Categorical(test_features[col])\n",
        "\n",
        "test_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IC1r2IRroc7w"
      },
      "outputs": [],
      "source": [
        "train_labels = train_labels.squeeze()\n",
        "test_labels = test_labels.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NYN6yzLZoc7w"
      },
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns for training models\n",
        "categorical_columns = train_features.select_dtypes(include=['object']).columns\n",
        "numerical_columns = train_features.select_dtypes(include=['int64', 'float64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GXy3rgNloc7x"
      },
      "outputs": [],
      "source": [
        "class CollinearityRemover(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.9):\n",
        "        self.threshold = threshold\n",
        "        self.to_remove_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Correlation matrix\n",
        "        corr_matrix = np.corrcoef(X, rowvar=False)\n",
        "        upper_triangle = np.triu(np.abs(corr_matrix), k=1)\n",
        "\n",
        "        # Highly correlated variables\n",
        "        self.to_remove_ = np.where(upper_triangle > self.threshold)[1]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Remove correlated variables\n",
        "        if self.to_remove_ is not None:\n",
        "            return np.delete(X, self.to_remove_, axis=1)\n",
        "        return X\n",
        "\n",
        "numerical_columns = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
        "categorical_columns = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex',\n",
        "                       'native.country.grouped', 'education.grouped']\n",
        "\n",
        "# Preprocessing\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('collinearity', CollinearityRemover(threshold=0.9))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_columns),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jysKArgeoc7x"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MylQsut8oc7x",
        "outputId": "6b236286-8153-4da6-b356-84ae04782a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
            "Best Parameters: {'model__C': 1}\n",
            "Confusion Matrix:\n",
            "[[4174  359]\n",
            " [ 601  899]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.90      4533\n",
            "           1       0.71      0.60      0.65      1500\n",
            "\n",
            "    accuracy                           0.84      6033\n",
            "   macro avg       0.79      0.76      0.77      6033\n",
            "weighted avg       0.83      0.84      0.84      6033\n",
            "\n",
            "Random Forest model saved as models/logistic_regression_model.pkl\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Pipeline: Preprocessing -> Feature Selection -> Model\n",
        "logistic_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('var_thresh', VarianceThreshold(threshold=0.01)),  # Rimuove variabili con varianza bassa\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__C': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    logistic_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "random_forest_filename = \"models/logistic_regression_model.pkl\"\n",
        "joblib.dump(logistic_pipeline, random_forest_filename)\n",
        "print(f\"Random Forest model saved as {random_forest_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgkF3aFWoc7y"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDCI4NuXoc7y",
        "outputId": "f1087852-e7ed-4d15-aa93-dac148ae60d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
            "Best Parameters: {'model__n_neighbors': 9, 'model__p': 2, 'model__weights': 'uniform'}\n",
            "Confusion Matrix:\n",
            "[[4104  429]\n",
            " [ 603  897]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89      4533\n",
            "           1       0.68      0.60      0.63      1500\n",
            "\n",
            "    accuracy                           0.83      6033\n",
            "   macro avg       0.77      0.75      0.76      6033\n",
            "weighted avg       0.82      0.83      0.83      6033\n",
            "\n",
            "KNN model saved as /content/drive/My Drive/models/knn_model.pkl\n"
          ]
        }
      ],
      "source": [
        "# KNN\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# Pipeline: Preprocessing -> Model\n",
        "knn_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'model__weights': ['uniform', 'distance'],\n",
        "    'model__p': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    knn_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "knn_filename = \"models/knn_model.pkl\"\n",
        "joblib.dump(knn_pipeline, knn_filename)\n",
        "print(f\"KNN model saved as {knn_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mnb1d5aoc7y"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcfa3MWZoc7y",
        "outputId": "fa658275-f2af-45c7-ab0c-dce01e2002c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
            "Best Parameters: {'model__var_smoothing': 1e-06}\n",
            "Confusion Matrix:\n",
            "[[3294 1239]\n",
            " [ 246 1254]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.73      0.82      4533\n",
            "           1       0.50      0.84      0.63      1500\n",
            "\n",
            "    accuracy                           0.75      6033\n",
            "   macro avg       0.72      0.78      0.72      6033\n",
            "weighted avg       0.82      0.75      0.77      6033\n",
            "\n",
            "Model saved as /content/drive/My Drive/models/naive_bayes_model.pkl\n"
          ]
        }
      ],
      "source": [
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('collinearity', CollinearityRemover(threshold=0.9))\n",
        "])\n",
        "\n",
        "# Necessary for Naive Bayes\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('encoder', OneHotEncoder(drop='first', sparse_output=False))  # Converti in formato denso\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_columns),\n",
        "        ('cat', categorical_pipeline, categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "naive_bayes_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('var_thresh', VarianceThreshold(threshold=0.01)),  # Rimuove variabili con varianza bassa\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    # Parametri di Naive Bayes\n",
        "    'model__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    naive_bayes_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "model_filename = \"models/naive_bayes_model.pkl\"\n",
        "joblib.dump(grid_search.best_estimator_, model_filename)\n",
        "print(f\"Model saved as {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZV9RxD0oc7y"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4XSZiGGoc7y",
        "outputId": "cf0f3cf0-76ed-4759-b3c4-9a47ae831ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
          ]
        }
      ],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Base model: Decision Tree\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Complete Pipeline\n",
        "bagging_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', bagging_model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 250, 500],\n",
        "    'model__max_samples': [0.5, 0.75, 1.0],\n",
        "    'model__max_features': [0.5, 0.75, 1.0],\n",
        "    'model__bootstrap': [True, False],\n",
        "    'model__bootstrap_features': [True, False]  # Bootstrap delle feature\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    bagging_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "bagging_filename = \"models/bagging_model.pkl\"\n",
        "joblib.dump(bagging_pipeline, bagging_filename)\n",
        "print(f\"Bagging model saved as {bagging_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8yTrSNqoc7y"
      },
      "source": [
        "## Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-wMkW0koc7y",
        "outputId": "ccbc4950-02b6-40ed-a6c8-087b80f45383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     29\u001b[0m     pipeline,\n\u001b[0;32m     30\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Addestramento del modello\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Migliori parametri trovati\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:783\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    782\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:908\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_scores_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# no need to fancy index w/ no subsampling\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_score_[i] \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    915\u001b[0m     verbose_reporter\u001b[38;5;241m.\u001b[39mupdate(i, \u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\sklearn\\_loss\\loss.py:400\u001b[0m, in \u001b[0;36mBaseLoss.__call__\u001b[1;34m(self, y_true, raw_prediction, sample_weight, n_threads)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, raw_prediction, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    381\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the weighted average loss.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m        Mean or averaged loss function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:568\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     result_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(a\u001b[38;5;241m.\u001b[39mdtype, wgt\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 568\u001b[0m scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\keita\\anaconda3\\envs\\income-classification\\lib\\site-packages\\numpy\\_core\\_methods.py:52\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Gradient Boosting\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "gradient_boosting_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'model__max_depth': [3, 5, 7],\n",
        "    'model__subsample': [0.8, 1.0],\n",
        "    'model__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    gradient_boosting_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "gradient_boosting_filename = \"models/gradient_boosting_model.pkl\"\n",
        "joblib.dump(gradient_boosting_pipeline, gradient_boosting_filename)\n",
        "print(f\"Gradient Boosting model saved as {gradient_boosting_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSc5YChpoc7z"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9c9Qirmoc7z",
        "outputId": "53bce824-9680-4bf2-861b-df14301c1d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n"
          ]
        }
      ],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "random_forest_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [None, 10, 20, 30],\n",
        "    'model__min_samples_split': [2, 5, 10],\n",
        "    'model__min_samples_leaf': [1, 2, 4],\n",
        "    'model__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    random_forest_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    scoring=make_scorer(f1_score, pos_label=1),\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "random_forest_filename = \"models/random_forest_model.pkl\"\n",
        "joblib.dump(random_forest_pipeline, random_forest_filename)\n",
        "print(f\"Random Forest model saved as {random_forest_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Mud3Dsoc7z"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVUbe2tCoc7z"
      },
      "outputs": [],
      "source": [
        "base_estimators = [\n",
        "    ('knn', knn_pipeline),\n",
        "    ('naive_bayes', naive_bayes_pipeline),\n",
        "    ('gradient_boosting', gradient_boosting_pipeline),\n",
        "    ('random_forest', random_forest_pipeline),\n",
        "    ('bagging', bagging_pipeline)\n",
        "]\n",
        "\n",
        "final_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Stacking classifier\n",
        "stacking_pipeline = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=final_model,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "stacking_pipeline.fit(train_features, train_labels)\n",
        "\n",
        "y_pred_stacking = stacking_pipeline.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred_stacking))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred_stacking))\n",
        "\n",
        "stacking_filename = \"models/stacking_model.pkl\"\n",
        "joblib.dump(stacking_pipeline, stacking_filename)\n",
        "print(f\"Stacking model saved as {stacking_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pL4QrUOxVfK"
      },
      "source": [
        "## Assessing with ROC curve and Lift Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xahs-tRgxrDs"
      },
      "outputs": [],
      "source": [
        "# ROC curves\n",
        "def plot_roc_curve(y_true, y_probs, model_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for y_prob, model_name in zip(y_probs, model_names):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Lift Curves\n",
        "def plot_lift_curve(y_true, y_probs, model_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for y_prob, model_name in zip(y_probs, model_names):\n",
        "        data = pd.DataFrame({'true': y_true, 'prob': y_prob})\n",
        "        data.sort_values('prob', ascending=False, inplace=True)\n",
        "        data['cumulative_positive'] = data['true'].cumsum()\n",
        "        data['cumulative_rate'] = np.arange(1, len(data) + 1) / len(data)\n",
        "        data['lift'] = data['cumulative_positive'] / data['cumulative_rate']\n",
        "\n",
        "        plt.plot(data['cumulative_rate'], data['lift'], lw=2, label=f'{model_name}')\n",
        "\n",
        "    plt.axhline(y=1, color='red', linestyle='--', label='Random Model')\n",
        "    plt.xlabel('Cumulative Percentage of Data')\n",
        "    plt.ylabel('Lift')\n",
        "    plt.title('Lift Curve')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Load saved models\n",
        "models = {\n",
        "    'Logistic Regression': joblib.load('logistic_regression_model.pkl'),\n",
        "    'KNN': joblib.load('knn_model.pkl'),\n",
        "    'Naive Bayes': joblib.load('naive_bayes_model.pkl'),\n",
        "    'Bagging': joblib.load('bagging_model.pkl'),\n",
        "    'Random Forest': joblib.load('random_forest_model.pkl'),\n",
        "    'Gradient Boosting': joblib.load('gradient_boosting_model.pkl'),\n",
        "    'Stacking': joblib.load('stacking_model.pkl'),\n",
        "}\n",
        "\n",
        "y_probs = []\n",
        "model_names = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_probs.append(model.predict_proba(test_features)[:, 1])\n",
        "    elif hasattr(model, 'decision_function'):\n",
        "        y_probs.append(model.decision_function(test_features))\n",
        "    model_names.append(name)\n",
        "\n",
        "# Compute and visualize Roc curves\n",
        "plot_roc_curve(test_labels, y_probs, model_names)\n",
        "\n",
        "# Compute and visualize Lift curves\n",
        "plot_lift_curve(test_labels, y_probs, model_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2C7Tv1F1356"
      },
      "source": [
        "## Choosing the right threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BnLGlNg163y"
      },
      "outputs": [],
      "source": [
        "# Simula il calcolo delle probabilità predette dalla Random Forest (esempio)\n",
        "y_true = test_labels  # Etichette vere\n",
        "y_scores = grid_search.predict_proba(test_features)[:, 1]  # Probabilità positive dal modello addestrato\n",
        "\n",
        "# Calcola la curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "\n",
        "# Specificità (1 - FPR) e Sensibilità (TPR)\n",
        "specificity = 1 - fpr\n",
        "sensitivity = tpr\n",
        "\n",
        "# Grafico Specificity e Sensitivity vs Threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, specificity, label='Specificity', color='dodgerblue')\n",
        "plt.plot(thresholds, sensitivity, label='Sensitivity', color='green')\n",
        "plt.axvline(x=0.4, color='red', linestyle='--', linewidth=2, label='Optimal Threshold')\n",
        "plt.text(0.42, 0.5, 'Optimal Threshold', color='red', fontsize=12)\n",
        "\n",
        "# Aggiungi titoli e legende\n",
        "plt.title(\"Performance Metrics vs Threshold\", fontsize=14)\n",
        "plt.xlabel(\"Threshold\", fontsize=12)\n",
        "plt.ylabel(\"Metrics\", fontsize=12)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jysKArgeoc7x",
        "rgkF3aFWoc7y",
        "7mnb1d5aoc7y",
        "n8yTrSNqoc7y",
        "I8Mud3Dsoc7z",
        "u2C7Tv1F1356"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "income-classification",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
